{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleção de recursos para aprendizado de máquina"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os recursos de dados que você usa para treinar seus modelos de aprendizado de máquina têm um enorme\n",
    "influência sobre o desempenho que você pode alcançar. Características irrelevantes ou parcialmente relevantes podem impactar negativamente desempenho do modelo. Neste capítulo, você descobrirá técnicas de seleção automática de recursos que você pode usar para preparar seus dados de aprendizado de máquina em Python com scikit-learn. \n",
    "\n",
    "1. Seleção Univariada.\n",
    "2. Eliminação de características recursivas.\n",
    "3. Análise de componentes principais.\n",
    "4. Importância do recurso."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seleção de Recursos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seleção de recursos é um processo em que você seleciona automaticamente os recursos em seus dados que contribuem mais para a variável de previsão ou saída na qual você está interessado. Tendo recursos irrelevantes em seus dados podem diminuir a precisão de muitos modelos, especialmente lineares algoritmos como regressão linear e logística. Três benefícios de executar a seleção de recursos antes de modelar seus dados são:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reduz o over tting: dados menos redundantes significam menos oportunidades para tomar decisões\n",
    "com base no ruído.\n",
    "* Melhora a precisão: Dados menos enganosos significam melhora na precisão da modelagem.\n",
    "* Reduz o tempo de treinamento: menos dados significa que os algoritmos treinam mais rápido.\n",
    "* Você pode aprender mais sobre a seleção de recursos com o scikit-learn no artigo Seleção de recursos1.\n",
    "* Cada receita de seleção de recurso usará o conjunto de dados de início do diabetes dos índios Pima."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seleção Univariada"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os testes estatísticos podem ser usados para selecionar os recursos que têm o relacionamento mais forte com a variável de saída. A biblioteca scikit-learn fornece o SelectKBest class2 que pode ser usado com um conjunto de diferentes testes estatísticos para selecionar um número específico de recursos. O exemplo abaixo usa o teste estatístico qui-quadrado (chi2) para recursos não negativos para selecionar 4 dos melhores características do conjunto de dados de diabetes dos índios Pima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 111.52  1411.887   17.605   53.108 2175.565  127.669    5.393  181.304]\n",
      "[[148.    0.   33.6  50. ]\n",
      " [ 85.    0.   26.6  31. ]\n",
      " [183.    0.   23.3  32. ]\n",
      " [ 89.   94.   28.1  21. ]\n",
      " [137.  168.   43.1  33. ]]\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction with Univariate Statistical Tests (Chi-squared for classification)\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# load data\n",
    "filename = 'datasets/diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres',\n",
    "         'skin', 'test', 'mass', \n",
    "         'pedi', 'age', 'class']\n",
    "\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# feature extraction\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(X, Y)\n",
    "\n",
    "# summarize scores\n",
    "set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "features = fit.transform(X)\n",
    "\n",
    "# summarize selected features\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode ver as pontuações de cada atributo e os 4 atributos escolhidos (aqueles com maior pontuações): plas, teste, massa e idade. Eu tenho os nomes para os atributos escolhidos manualmente mapeando o índice das 4 pontuações mais altas para o índice dos nomes dos atributos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminação de recursos recursivos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Eliminação Recursiva de Recursos (ou RFE) funciona removendo recursivamente atributos e construindo um modelo sobre os atributos que permanecem. Ele usa a precisão do modelo para identificar quais atributos (e combinação de atributos) contribuem mais para prever o atributo de destino. Você pode aprender mais sobre o RFE class3 na documentação do scikit-learn. O exemplo abaixo usa RFE com o algoritmo de regressão logística para selecionar os 3 principais recursos. A escolha de algoritmo não importa muito, desde que seja hábil e consistente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RFE.__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\GIT-repository\\github-machine-learning-mastery\\machine-learning-mastery\\07-Seleção_de_recursos_para_ML.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-mastery/machine-learning-mastery/07-Sele%C3%A7%C3%A3o_de_recursos_para_ML.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# feature extraction\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-mastery/machine-learning-mastery/07-Sele%C3%A7%C3%A3o_de_recursos_para_ML.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m model \u001b[39m=\u001b[39m LogisticRegression()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-mastery/machine-learning-mastery/07-Sele%C3%A7%C3%A3o_de_recursos_para_ML.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m rfe \u001b[39m=\u001b[39m RFE(model, \u001b[39m3\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-mastery/machine-learning-mastery/07-Sele%C3%A7%C3%A3o_de_recursos_para_ML.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m fit \u001b[39m=\u001b[39m rfe\u001b[39m.\u001b[39mfit(X, Y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-mastery/machine-learning-mastery/07-Sele%C3%A7%C3%A3o_de_recursos_para_ML.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNum Features: \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39m%\u001b[39m fit\u001b[39m.\u001b[39mn_features_\n",
      "\u001b[1;31mTypeError\u001b[0m: RFE.__init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# Feature Extraction with RFE\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# load data\n",
    "filename = 'datasets/diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres',\n",
    "         'skin', 'test', 'mass', \n",
    "         'pedi', 'age', 'class']\n",
    "\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# feature extraction\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 3)\n",
    "fit = rfe.fit(X, Y)\n",
    "\n",
    "print(\"Num Features: %d\") % fit.n_features_\n",
    "print(\"Selected Features: %s\") % fit.support_\n",
    "print(\"Feature Ranking: %s\") % fit.ranking_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode ver que o RFE escolheu os 3 principais recursos como preg, mass e pedi. Estes são marcados Verdadeiro na matriz de suporte e marcado com a opção 1 na matriz de classificação. Novamente, você pode mapeie manualmente os índices de recursos para os índices de nomes de atributos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise do componente principal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Análise de Componentes Principais (ou PCA) usa álgebra linear para transformar o conjunto de dados em um forma comprimida. Geralmente, isso é chamado de técnica de redução de dados. Uma propriedade do PCA é que você pode escolher o número de dimensões ou componentes principais no resultado transformado. Em No exemplo abaixo, usamos o PCA e selecionamos 3 componentes principais. Saiba mais sobre o PCA class no scikit-learn revisando o API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction with PCA\n",
    "from pandas import read_csv\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# load data\n",
    "filename = 'datasets/diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres',\n",
    "         'skin', 'test', 'mass', \n",
    "         'pedi', 'age', 'class']\n",
    "\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# feature extraction\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X)\n",
    "\n",
    "# summarize components\n",
    "print(\"Explained Variance: %s\") % fit.explained_variance_ratio_\n",
    "print(fit.components_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode ver que o conjunto de dados transformado (3 componentes principais) apresenta pouca semelhança aos dados de origem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importância do recurso"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Árvores de decisão ensacadas como Random Forest e Extra Trees podem ser usadas para estimar a importância de características. No exemplo abaixo construímos um classificador ExtraTreesClassifier para o Pima Conjunto de dados de início de diabetes na Índia. Você pode aprender mais sobre a classe ExtraTreesClassifier na API scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance with Extra Trees Classifier\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# load data\n",
    "filename = 'datasets/diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres',\n",
    "         'skin', 'test', 'mass', \n",
    "         'pedi', 'age', 'class']\n",
    "\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# feature extraction\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, Y)\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode ver que recebemos uma pontuação de importância para cada atributo onde quanto maior o pontuação, mais importante é o atributo. As pontuações sugerem a importância de plas, idade e massa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Caique Miranda\" -gu \"caiquemiranda\" -iv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
