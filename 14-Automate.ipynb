{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatize fluxos de trabalho de aprendizado de máquina com pipelines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem trabalhos padrão\n",
    "fluxos em um projeto de aprendizado de máquina que pode ser automatizado. Em Python\n",
    "scikit-learn, Pipelines ajudam a de nir e automatizar claramente esses trabalhos\n",
    "ows. Neste capítulo você\n",
    "descobrirá Pipelines no scikit-learn e como você pode automatizar o aprendizado de máquina comum\n",
    "trabalhar\n",
    "ows. Depois de concluir esta lição, você saberá:\n",
    "1. Como usar pipelines para minimizar o vazamento de dados.\n",
    "2. Como construir um pipeline de modelagem e preparação de dados.\n",
    "3. Como construir um pipeline de modelagem e extração de recursos.\n",
    "Vamos começar."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como automatizar fluxos de trabalho de aprendizado de máquina"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem trabalhos padrão\n",
    "fluxos em aprendizado de máquina aplicado. Padrão porque superam\n",
    "problemas comuns como vazamento de dados em seu equipamento de teste. Python scikit-learn fornece um pipeline\n",
    "utilitário para ajudar a automatizar o trabalho de aprendizado de máquina\n",
    "ows. Os dutos funcionam permitindo uma linha\n",
    "sequência de transformações de dados a serem encadeadas, culminando em um processo de modelagem que pode\n",
    "ser avaliado."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo é garantir que todas as etapas do pipeline sejam restritas aos dados disponíveis\n",
    "para a avaliação, como o conjunto de dados de treinamento ou cada dobra do procedimento de validação cruzada.\n",
    "Você pode aprender mais sobre Pipelines no scikit-learn lendo a seção Pipeline1 do usuário\n",
    "guia. Você também pode revisar a documentação da API para as classes Pipeline e FeatureUnion\n",
    "e o módulo de pipeline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparação de dados e pipeline de modelagem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma armadilha fácil de cair no aprendizado de máquina aplicado é vazar dados do seu conjunto de dados de treinamento\n",
    "ao seu conjunto de dados de teste. Para evitar essa armadilha, você precisa de um equipamento de teste robusto com forte separação de treinamento e teste. Isso inclui a preparação de dados. A preparação de dados é uma maneira fácil de vazar\n",
    "conhecimento de todo o conjunto de dados de treinamento para o algoritmo. Por exemplo, preparando seus dados\n",
    "usar normalização ou padronização em todo o conjunto de dados de treinamento antes do aprendizado não\n",
    "ser um teste válido porque o conjunto de dados de treinamento estaria em\n",
    "uenciado pela escala dos dados\n",
    "no conjunto de teste."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os pipelines ajudam a evitar o vazamento de dados em seu equipamento de teste, garantindo que a preparação de dados\n",
    "como a padronização é restrita a cada dobra do seu procedimento de validação cruzada. O exemplo\n",
    "abaixo demonstra este importante trabalho de preparação de dados e avaliação de modelo\n",
    "olá no\n",
    "Conjunto de dados de início do diabetes dos índios Pima. O pipeline é definido com duas etapas:\n",
    "\n",
    "1. Padronize os dados.\n",
    "2. Aprenda um modelo de Análise Discriminante Linear.\n",
    "   \n",
    "O pipeline é então avaliado usando validação cruzada de 10 vezes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that standardizes the data then creates a model\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# load data\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "# create pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('lda', LinearDiscriminantAnalysis()))\n",
    "model = Pipeline(estimators)\n",
    "# evaluate pipeline\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe como criamos uma lista Python de etapas que são fornecidas ao Pipeline para o processo\n",
    "os dados. Observe também como o próprio Pipeline é tratado como um estimador e é avaliado em seu\n",
    "inteiramente pelo procedimento de validação cruzada k-fold. A execução do exemplo fornece um resumo de\n",
    "precisão da configuração no conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extração de recursos e pipeline de modelagem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A extração de recursos é outro procedimento suscetível ao vazamento de dados. Como a preparação de dados,\n",
    "os procedimentos de extração de recursos devem ser restritos aos dados em seu conjunto de dados de treinamento. O\n",
    "O pipeline fornece uma ferramenta útil chamada FeatureUnion, que permite os resultados de vários\n",
    "procedimentos de seleção e extração de recursos a serem combinados em um conjunto de dados maior no qual um\n",
    "modelo pode ser treinado. É importante ressaltar que toda a extração de recursos e a união de recursos ocorre\n",
    "dentro de cada dobra do procedimento de validação cruzada. O exemplo abaixo demonstra o pipeline\n",
    "de nido com quatro etapas:\n",
    "1. Extração de Características com Análise de Componentes Principais (3 características).\n",
    "2. Extração de características com seleção estatística (6 características).\n",
    "3. União de recursos.\n",
    "4. Aprenda um modelo de regressão logística.\n",
    "O pipeline é então avaliado usando validação cruzada de 10 vezes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that extracts features from the data then creates a model\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "# load data\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "# create feature union\n",
    "features = []\n",
    "features.append(('pca', PCA(n_components=3)))\n",
    "features.append(('select_best', SelectKBest(k=6)))\n",
    "feature_union = FeatureUnion(features)\n",
    "# create pipeline\n",
    "estimators = []\n",
    "estimators.append(('feature_union', feature_union))\n",
    "estimators.append(('logistic', LogisticRegression()))\n",
    "model = Pipeline(estimators)\n",
    "# evaluate pipeline\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe como o FeatureUnion é seu próprio Pipeline que por sua vez é uma única etapa no nal\n",
    "Pipeline utilizado para alimentar a Regressão Logística. Isso pode fazer você pensar em como começar\n",
    "incorporando pipelines dentro de pipelines. A execução do exemplo fornece um resumo da precisão de\n",
    "a configuração no conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Caique Miranda\" -gu \"caiquemiranda\" -iv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
