{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melhore o desempenho com conjuntos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os conjuntos podem aumentar a precisão do seu conjunto de dados. Neste capítulo você descobrirá\n",
    "como você pode criar alguns dos tipos mais poderosos de ensembles em Python usando o scikit-learn.\n",
    "Esta lição irá guiá-lo através de Boosting, Bagging e Votação da Maioria e mostrará como você pode\n",
    "pode continuar aumentando a precisão dos modelos em seus próprios conjuntos de dados. Depois de completar\n",
    "nesta lição você saberá:\n",
    " \n",
    "1. Como usar métodos bagging ensemble, como árvores de decisão ensacadas, floresta aleatória e\n",
    "árvores extras.\n",
    "2. Como usar métodos de conjunto de reforço, como AdaBoost e aumento de gradiente estocástico.\n",
    "3. Como usar métodos de combinação de votação para combinar as previsões de vários algoritmos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine modelos em previsões de conjunto"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os três métodos mais populares para combinar as previsões de diferentes modelos são:\n",
    "Ensacamento. Construir vários modelos (normalmente do mesmo tipo) a partir de diferentes subamostras\n",
    "do conjunto de dados de treinamento.\n",
    "Impulsionando. Construir vários modelos (normalmente do mesmo tipo), cada um dos quais aprende a\n",
    "x os erros de previsão de um modelo anterior na sequência de modelos.\n",
    "Votação. Construindo modelos múltiplos (normalmente de tipos diferentes) e estatísticas simples (como\n",
    "cálculo da média) são usados para combinar previsões.\n",
    "Isso pressupõe que você geralmente esteja familiarizado com algoritmos e ensemble de aprendizado de máquina\n",
    "métodos e não entrará em detalhes de como os algoritmos funcionam ou seus parâmetros.\n",
    "O conjunto de dados do início do diabetes dos índios Pima é usado para demonstrar cada algoritmo. Cada\n",
    "algoritmo de conjunto é demonstrado usando validação cruzada de 10 vezes e a precisão de classificação\n",
    "métrica de desempenho."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmos de ensacamento(Bagging )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap Aggregation (ou Bagging) envolve a coleta de várias amostras de seu conjunto de dados de treinamento\n",
    "(com reposição) e treinando um modelo para cada amostra. A previsão de saída nal é calculada em média\n",
    "através das previsões de todos os sub-modelos. Os três modelos de ensacamento abordados nesta seção\n",
    "são como segue:\n",
    "\n",
    "Árvores de Decisão Bagged.\n",
    "Floresta Aleatória.\n",
    "Árvores extras."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Árvores de Decisão Bagged"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O ensacamento funciona melhor com algoritmos que possuem alta variância. Um exemplo popular são\n",
    "árvores de decisão, muitas vezes construídas sem poda. No exemplo abaixo está um exemplo\n",
    "de usar o BaggingClassifier com o algoritmo Classi cation and Regression Trees\n",
    "(DecisionTreeClassifier1). Um total de 100 árvores são criadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7578263841421736\n"
     ]
    }
   ],
   "source": [
    "# Bagged Decision Trees for Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "filename = 'datasets/pima-indians-diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres', \n",
    "         'skin', 'test', 'mass',\n",
    "         'pedi', 'age', 'class']\n",
    "\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "kfold = KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executando o exemplo, obtemos uma estimativa robusta da precisão do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7578263841421736\n"
     ]
    }
   ],
   "source": [
    "print(results.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests é uma extensão das árvores de decisão ensacadas. As amostras do conjunto de dados de treinamento são\n",
    "tomadas com reposição, mas as árvores são construídas de forma a reduzir a correlação\n",
    "entre classificadores individuais. Especificamente, em vez de escolher avidamente o melhor ponto de divisão em\n",
    "Na construção de cada árvore, apenas um subconjunto aleatório de recursos é considerado para cada divisão. Você\n",
    "pode construir um modelo Random Forest para classificação usando o RandomForestClassifier\n",
    "classe2. O exemplo abaixo demonstra o uso de Random Forest para classificação com 100 árvores\n",
    "e pontos de divisão escolhidos de uma seleção aleatória de 3 recursos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7708646616541354\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "filename = 'datasets/pima-indians-diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres',\n",
    "         'skin', 'test', 'mass',\n",
    "         'pedi', 'age', 'class']\n",
    "\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A execução do exemplo fornece uma estimativa média da precisão da classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7708646616541354\n"
     ]
    }
   ],
   "source": [
    "print(results.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra Trees"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Trees são outra modificação de bagging onde árvores aleatórias são construídas a partir de\n",
    "amostras do conjunto de dados de treinamento. Você pode construir um modelo Extra Trees para classificação usando\n",
    "a classe ExtraTreesClassifier3. O exemplo abaixo fornece uma demonstração de árvores extras\n",
    "com o número de árvores definido como 100 e divisões escolhidas entre 7 recursos aleatórios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7630382775119617\n"
     ]
    }
   ],
   "source": [
    "# Extra Trees Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "filename = 'datasets/pima-indians-diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres',\n",
    "         'skin', 'test', 'mass',\n",
    "         'pedi', 'age', 'class']\n",
    "\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_trees = 100\n",
    "max_features = 7\n",
    "kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A execução do exemplo fornece uma estimativa média da precisão da classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7630382775119617\n"
     ]
    }
   ],
   "source": [
    "print(results.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting Algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O reforço de algoritmos de conjunto cria uma sequência de modelos que tentam corrigir os erros\n",
    "dos modelos antes deles na sequência. Uma vez criados, os modelos fazem previsões que\n",
    "podem ser ponderados por sua precisão demonstrada e os resultados são combinados para criar um resultado final\n",
    "previsão de saída. Os dois algoritmos de aprendizado de máquina de conjunto de reforço mais comuns são:\n",
    "AdaBoost.\n",
    "Reforço de gradiente estocástico."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AdaBoost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost foi talvez o primeiro algoritmo de conjunto de boosting bem-sucedido. geralmente funciona\n",
    "ponderando as instâncias no conjunto de dados pela facilidade ou dificuldade de classificá-las, permitindo\n",
    "o algoritmo para prestar ou menos atenção a eles na construção de modelos subsequentes. Você\n",
    "pode construir um modelo AdaBoost para classificação usando o AdaBoostClassifier class4. O\n",
    "exemplo abaixo demonstra a construção de 30 árvores de decisão em sequência usando o AdaBoost\n",
    "algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7552802460697198\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "filename = 'datasets/pima-indians-diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres', \n",
    "         'skin', 'test', 'mass',\n",
    "         'pedi', 'age', 'class']\n",
    "\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_trees = 30\n",
    "seed=7\n",
    "kfold = KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A execução do exemplo fornece uma estimativa média da precisão da classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7552802460697198\n"
     ]
    }
   ],
   "source": [
    "print(results.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stochastic Gradient Boosting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Stochastic Gradient Boosting (também chamado de Gradient Boosting Machines) é um dos métodos mais\n",
    "técnicas sofisticadas de ensemble. É também uma técnica que está a revelar-se talvez uma das\n",
    "as melhores técnicas disponíveis para melhorar o desempenho através de ensembles. Você pode construir um\n",
    "Modelo Gradient Boosting para classi cação usando a classe GradientBoostingClassifier5. O\n",
    "O exemplo abaixo demonstra o Stochastic Gradient Boosting para classificação com 100 árvores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7591934381408066\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Boosting Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "filename = 'datasets/pima-indians-diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres',\n",
    "         'skin', 'test', 'mass',\n",
    "         'pedi', 'age', 'class']\n",
    "\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "kfold = KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A execução do exemplo fornece uma estimativa média da precisão da classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7591934381408066\n"
     ]
    }
   ],
   "source": [
    "print(results.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting Ensemble"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A votação é uma das maneiras mais simples de combinar as previsões de vários aprendizados de máquina\n",
    "algoritmos. Ele funciona primeiro criando dois ou mais modelos autônomos de seu conjunto de dados de treinamento.\n",
    "Um classificador de votação pode então ser usado para agrupar seus modelos e calcular a média das previsões do\n",
    "submodelos quando solicitado a fazer previsões para novos dados. As previsões dos sub-modelos podem\n",
    "ser ponderados, mas especificar os pesos para classificadores manualmente ou mesmo heuristicamente é difícil.\n",
    "Métodos mais avançados podem aprender como ponderar melhor as previsões de submodelos, mas isso\n",
    "é chamado de empilhamento (agregação empilhada) e atualmente não é fornecido no scikit-learn."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode criar um modelo de conjunto de votação para classificação usando o VotingClassifier\n",
    "classe6. O código abaixo fornece um exemplo de combinação das previsões de regressão logística,\n",
    "árvores de classificação e regressão e máquinas de vetores de suporte juntas para uma classificação\n",
    "problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.769583048530417\n"
     ]
    }
   ],
   "source": [
    "# Voting Ensemble for Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "filename = 'datasets/pima-indians-diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres',\n",
    "         'skin', 'test', 'mass',\n",
    "         'pedi', 'age', 'class']\n",
    "\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "\n",
    "# create the sub models\n",
    "estimators = []\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(('logistic', model1))\n",
    "model2 = DecisionTreeClassifier()\n",
    "estimators.append(('cart', model2))\n",
    "model3 = SVC()\n",
    "estimators.append(('svm', model3))\n",
    "\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "results = cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A execução do exemplo fornece uma estimativa média da precisão da classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.769583048530417\n"
     ]
    }
   ],
   "source": [
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Github username: caiquemiranda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -gu \"caiquemiranda\" -iv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
