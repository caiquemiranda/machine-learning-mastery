{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare seus dados para aprendizado de máquina"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muitos algoritmos de aprendizado de máquina fazem suposições sobre seus dados. Muitas vezes é muito bom idéia de preparar seus dados de forma a melhor expor a estrutura do problema para a máquina lgoritmos de aprendizado que você pretende usar. Neste capítulo, você descobrirá como preparar seus dados para aprendizado de máquina em Python usando o scikit-learn. \n",
    "\n",
    "1. Redimensione os dados.\n",
    "2. Padronize os dados.\n",
    "3. Normalize os dados.\n",
    "4. Binarize os dados."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Necessidade de pré-processamento de dados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você quase sempre precisa pré-processar seus dados. É uma etapa necessária. Uma dificuldade é que algoritmos diferentes fazem suposições diferentes sobre seus dados e podem exigir transforma. Além disso, quando você segue todas as regras e prepara seus dados, às vezes os algoritmos pode fornecer melhores resultados sem pré-processamento."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geralmente, eu recomendaria criar muitas exibições e transformações diferentes de seus dados, em seguida, exercite alguns algoritmos em cada exibição de seu conjunto de dados. Isso irá ajudá-lo a ush descubra quais transformações de dados podem ser melhores para expor a estrutura do seu problema em geral."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformações de dados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta lição, você trabalhará com 4 receitas diferentes de pré-processamento de dados para aprendizado de máquina. O conjunto de dados de diabetes Pima Indian é usado em cada receita. Cada receita segue a mesma estrutura: Carregue o conjunto de dados de um URL. Divida o conjunto de dados nas variáveis de entrada e saída para aprendizado de máquina. Aplique uma transformação de pré-processamento às variáveis de entrada. Resumir os dados para mostrar a alteração."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A biblioteca scikit-learn fornece dois idiomas padrão para transformar dados. cada um é útil em diferentes circunstâncias. As transformações são calculadas de forma que possam ser aplicadas aos seus dados de treinamento e quaisquer amostras de dados que você possa ter no futuro. O scikit-learn A documentação contém algumas informações sobre como usar vários métodos diferentes de pré-processamento:\n",
    "\n",
    "- Ajuste e transformação múltipla.\n",
    "- Ajuste e transformação combinados."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O método Fit and Multiple Transform é a abordagem preferida. Você chama o fit() função para preparar os parâmetros da transformação uma vez em seus dados. Então mais tarde você pode usar a função transform() nos mesmos dados para prepará-los para modelagem e novamente no teste ou conjunto de dados de validação ou novos dados que você pode ver no futuro. A combinação de ajuste e transformação é uma conveniência que você pode usar para uma ou outras tarefas. Isso pode ser útil se você estiver interessado em plotar ou resumir os dados transformados. Você pode revisar a API de pré-processamento em scikit-learn aqui1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redimensionar dados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando seus dados são compostos de atributos com escalas variadas, muitos algoritmos de aprendizado de máquina pode se beneficiar do redimensionamento dos atributos para que todos tenham a mesma escala. Muitas vezes isso é referido como normalização e atributos são frequentemente redimensionados no intervalo entre 0 e 1. Isso é útil para algoritmos de otimização usados no núcleo de algoritmos de aprendizado de máquina como gradiente descida. Também é útil para algoritmos que ponderam entradas como regressão e redes neurais e algoritmos que usam medidas de distância como k-Nearest Neighbors. Você pode redimensionar seus dados usando scikit-learn usando o MinMaxScaler class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale data (between 0 and 1)\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "filename = 'datasets/diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres',\n",
    "         'skin', 'test', 'mass', \n",
    "         'pedi', 'age', 'class']\n",
    "\n",
    "dataframe = read_csv(filename, \n",
    "                     names=names)\n",
    "array = dataframe.values\n",
    "\n",
    "# separate array into input and output components\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "\n",
    "# summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padronizar dados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A padronização é uma técnica útil para transformar atributos com uma distribuição Gaussiana e médias diferentes e desvios padrão para uma distribuição Gaussiana padrão com uma média de 0 e um desvio padrão de 1. É mais adequado para técnicas que assumem uma Gaussiana distribuição nas variáveis de entrada e funcionam melhor com dados reescalonados, como regressão linear, regressão logística e análise linear discriminada. Você pode padronizar dados usando o scikit-learn com a classe StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data (0 mean, 1 stdev)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "\n",
    "filename = 'datasets/diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres',\n",
    "         'skin', 'test', 'mass', \n",
    "         'pedi', 'age', 'class']\n",
    "\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "\n",
    "# separate array into input and output components\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "rescaledX = scaler.transform(X)\n",
    "\n",
    "# summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizar Dados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A normalização no scikit-learn refere-se ao redimensionamento de cada observação (linha) para ter um comprimento de 1 (chamado uma norma unitária ou um vetor com o comprimento de 1 em álgebra linear). Este método de pré-processamento pode ser útil para conjuntos de dados esparsos (muitos zeros) com atributos de escalas variadas ao usar algoritmos que ponderam valores de entrada, como redes neurais e algoritmos que usam distância medidas como k-vizinhos mais próximos. Você pode normalizar dados em Python com scikit-learn usando o normalizador class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data (length of 1)\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "\n",
    "filename = 'datasets/diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres',\n",
    "         'skin', 'test', 'mass', \n",
    "         'pedi', 'age', 'class']\n",
    "\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "\n",
    "# separate array into input and output components\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "scaler = Normalizer().fit(X)\n",
    "normalizedX = scaler.transform(X)\n",
    "\n",
    "# summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(normalizedX[0:5,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarizar Dados (Tornar Binário)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode transformar seus dados usando um limite binário. Todos os valores acima do limite são marcados como 1 e todos iguais ou abaixo são marcados como 0. Isso é chamado de binarização de seus dados ou limitando seus dados. Pode ser útil quando você tem probabilidades de querer fazer nítidos valores. Também é útil na engenharia de recursos e você deseja adicionar novos recursos que indicam algo significativo. Você pode criar novos atributos binários em Python usando scikit-learn com a classe binarizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarization\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "# separate array into input and output components\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "binarizer = Binarizer(threshold=0.0).fit(X)\n",
    "binaryX = binarizer.transform(X)\n",
    "# summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(binaryX[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Caique Miranda\" -gu \"caiquemiranda\" -iv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
